#!/usr/bin/env python3
"""
HTTPä»£ç†æœåŠ¡å™¨ - å®Œæ•´ä¿®å¤ç‰ˆ
è¿è¡Œç«¯å£: 60000
åŒæ—¶ä¿®å¤æ–°é—»è·³è½¬å’Œæœç´¢é—®é¢˜
"""

import http.server
import socketserver
import urllib.parse
import requests
from bs4 import BeautifulSoup
import re
import time

class CompleteFixProxyHandler(http.server.BaseHTTPRequestHandler):
    """å®Œæ•´ä¿®å¤çš„ä»£ç†å¤„ç†å™¨ - åŒæ—¶ä¿®å¤æ–°é—»è·³è½¬å’Œæœç´¢é—®é¢˜"""

    config = {
        'port': 60000,
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    def do_GET(self):
        """å¤„ç†GETè¯·æ±‚"""
        try:
            print(f"GETè¯·æ±‚: {self.path}")
            
            if self.path == '/':
                self._serve_homepage()
            elif self.path.startswith('/proxy?url='):
                self._proxy_webpage()
            elif self.path.startswith('/proxy?') and 'url=' not in self.path:
                # å…³é”®ä¿®å¤ï¼šå¤„ç†æœç´¢åç”Ÿæˆçš„URLï¼ˆæ²¡æœ‰urlå‚æ•°çš„æƒ…å†µï¼‰
                self._handle_search_result()
            else:
                self._proxy_resource()
        except Exception as e:
            self.send_error(500, "Server Error: " + str(e))

    def do_POST(self):
        """å¤„ç†POSTè¯·æ±‚"""
        try:
            if self.path.startswith('/proxy?url='):
                self._proxy_post_request_fixed()
            else:
                self.send_error(404, "Not Found")
        except Exception as e:
            self.send_error(500, "Server Error: " + str(e))

    def _handle_search_result(self):
        """ä¸“é—¨å¤„ç†æœç´¢åç”Ÿæˆçš„URL - å…³é”®ä¿®å¤æœç´¢é—®é¢˜"""
        try:
            # è§£æå½“å‰è·¯å¾„ä¸­çš„å‚æ•°
            parsed_path = urllib.parse.urlparse(self.path)
            query_params = urllib.parse.parse_qs(parsed_path.query)
            
            print(f"æœç´¢å‚æ•°: {query_params}")
            
            # ä»Refererä¸­è·å–åŸå§‹æœç´¢é¡µé¢URL
            referer = self.headers.get('Referer', '')
            if referer and '/proxy?url=' in referer:
                # æå–åŸå§‹æœç´¢é¡µé¢çš„URL
                referer_parsed = urllib.parse.urlparse(referer)
                referer_query = urllib.parse.parse_qs(referer_parsed.query)
                base_search_url = referer_query.get('url', [''])[0]
                
                if base_search_url:
                    # æ„å»ºæœç´¢ç»“æœçš„å®Œæ•´URL
                    # å¯¹äº360æœç´¢ï¼Œæœç´¢ç»“æœé¡µé€šå¸¸æ˜¯ www.so.com/s åŠ ä¸Šå‚æ•°
                    search_result_url = "https://www.so.com/s"
                    
                    # æ·»åŠ æ‰€æœ‰æœç´¢å‚æ•°
                    if query_params:
                        search_result_url += "?" + urllib.parse.urlencode(query_params, doseq=True)
                    
                    print(f"æ„å»ºæœç´¢ç»“æœURL: {search_result_url}")
                    
                    # ä»£ç†è¿™ä¸ªæœç´¢ç»“æœé¡µé¢
                    self._proxy_specific_url(search_result_url)
                    return
            
            # å¦‚æœæ— æ³•ä»Refererè·å–ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            self._try_auto_fix_search()
            
        except Exception as e:
            print(f"æœç´¢å¤„ç†é”™è¯¯: {str(e)}")
            self.send_error(500, "Search processing error: " + str(e))

    def _try_auto_fix_search(self):
        """å°è¯•è‡ªåŠ¨ä¿®å¤æœç´¢URL"""
        try:
            # è§£æå½“å‰è·¯å¾„
            parsed_path = urllib.parse.urlparse(self.path)
            query_params = urllib.parse.parse_qs(parsed_path.query)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœç´¢å‚æ•°
            if 'q' in query_params or 'query' in query_params or 'keyword' in query_params:
                # å‡è®¾æ˜¯360æœç´¢
                search_result_url = "https://www.so.com/s"
                
                if query_params:
                    search_result_url += "?" + urllib.parse.urlencode(query_params, doseq=True)
                
                print(f"è‡ªåŠ¨ä¿®å¤æœç´¢URL: {search_result_url}")
                self._proxy_specific_url(search_result_url)
                return
            
            # å¦‚æœæ— æ³•ä¿®å¤ï¼Œè¿”å›é”™è¯¯
            self.send_error(400, "æ— æ³•å¤„ç†çš„æœç´¢è¯·æ±‚")
            
        except Exception as e:
            print(f"è‡ªåŠ¨ä¿®å¤é”™è¯¯: {str(e)}")
            self.send_error(500, "Auto-fix error: " + str(e))

    def _proxy_specific_url(self, target_url):
        """ä»£ç†ç‰¹å®šURL"""
        print(f"ä»£ç†ç‰¹å®šURL: {target_url}")
        
        headers = {
            'User-Agent': self.config['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
        
        try:
            response = requests.get(target_url, headers=headers, timeout=30, verify=False)
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to fetch: " + str(e))
            return
        
        if response.status_code != 200:
            self.send_error(response.status_code, "Target returned " + str(response.status_code))
            return
        
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' in content_type:
            rewritten_content = self._rewrite_html_content_complete(response.text, target_url)
            self.send_response(200)
            self.send_header('Content-Type', 'text/html; charset=utf-8')
            self.send_header('Content-Length', str(len(rewritten_content.encode('utf-8'))))
            self.end_headers()
            self.wfile.write(rewritten_content.encode('utf-8'))
        else:
            self._proxy_raw_content(response)

    def _serve_homepage(self):
        """æä¾›ä»£ç†ä¸»é¡µç•Œé¢"""
        homepage_html = '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>å®Œæ•´ä¿®å¤ä»£ç† - 60000ç«¯å£</title>
            <meta charset="utf-8">
            <style>
                body { 
                    font-family: Arial, sans-serif; 
                    margin: 40px; 
                    background: #f5f5f5;
                }
                .container { 
                    max-width: 800px; 
                    margin: 0 auto; 
                    background: white; 
                    padding: 30px; 
                    border-radius: 10px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }
                h1 { 
                    color: #333; 
                    text-align: center;
                }
                .form-group { 
                    margin: 20px 0; 
                }
                input[type="url"] { 
                    width: 100%; 
                    padding: 12px; 
                    border: 1px solid #ddd; 
                    border-radius: 5px; 
                    font-size: 16px;
                    box-sizing: border-box;
                }
                button { 
                    background: #007cba; 
                    color: white; 
                    border: none; 
                    padding: 12px 24px; 
                    border-radius: 5px; 
                    cursor: pointer; 
                    font-size: 16px;
                    width: 100%;
                }
                button:hover { 
                    background: #005a87; 
                }
                .info {
                    background: #e7f3ff;
                    padding: 15px;
                    border-radius: 5px;
                    margin: 20px 0;
                    border-left: 4px solid #007cba;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸŒ å®Œæ•´ä¿®å¤ä»£ç†æœåŠ¡</h1>
                <div class="info">
                    <strong>ä¿®å¤å†…å®¹ï¼š</strong> åŒæ—¶è§£å†³æ–°é—»ç›´æ¥è·³è½¬å’Œæœç´¢é—®é¢˜ã€‚
                </div>
                <form action="/proxy" method="GET">
                    <div class="form-group">
                        <input type="url" name="url" placeholder="https://www.example.com" required value="https://www.so.com">
                    </div>
                    <button type="submit">å¼€å§‹ä»£ç†è®¿é—®</button>
                </form>
                <div style="margin-top: 20px; text-align: center; color: #666;">
                    <small>ä»£ç†æœåŠ¡è¿è¡Œåœ¨ç«¯å£ 60000 | æ–°é—»è·³è½¬ä¸æœç´¢é—®é¢˜å·²ä¿®å¤</small>
                </div>
            </div>
        </body>
        </html>
        '''
        
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.send_header('Content-Length', str(len(homepage_html.encode('utf-8'))))
        self.end_headers()
        self.wfile.write(homepage_html.encode('utf-8'))

    def _proxy_webpage(self):
        """ä»£ç†å¹¶é‡å†™ç½‘é¡µå†…å®¹"""
        query_params = urllib.parse.parse_qs(urllib.parse.urlparse(self.path).query)
        target_url = query_params.get('url', [''])[0]

        if not target_url:
            self.send_error(400, "Missing URL parameter")
            return

        if not target_url.startswith(('http://', 'https://')):
            target_url = 'http://' + target_url

        headers = {
            'User-Agent': self.config['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        try:
            response = requests.get(target_url, headers=headers, timeout=30, verify=False)
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to fetch target website: " + str(e))
            return

        if response.status_code != 200:
            self.send_error(response.status_code, "Target website returned " + str(response.status_code))
            return

        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' in content_type:
            rewritten_content = self._rewrite_html_content_complete(response.text, target_url)
            self.send_response(200)
            self.send_header('Content-Type', 'text/html; charset=utf-8')
            self.send_header('Content-Length', str(len(rewritten_content.encode('utf-8'))))
            self.end_headers()
            self.wfile.write(rewritten_content.encode('utf-8'))
        else:
            self._proxy_raw_content(response)

    def _proxy_resource(self):
        """ä»£ç†é™æ€èµ„æº - å¢å¼ºé”™è¯¯å¤„ç†"""
        referer = self.headers.get('Referer', '')

        if '/proxy?url=' in referer:
            referer_parsed = urllib.parse.urlparse(referer)
            referer_query = urllib.parse.parse_qs(referer_parsed.query)
            base_url = referer_query.get('url', [''])[0]

            if base_url:
                # å¤„ç†è¿”å›ä¸»é¡µçš„ç‰¹æ®Šæƒ…å†µ
                if self.path == '/':
                    self._serve_homepage()
                    return
                    
                resource_url = urllib.parse.urljoin(base_url, self.path)

                headers = {
                    'User-Agent': self.config['user_agent'],
                    'Referer': base_url
                }

                try:
                    response = requests.get(resource_url, headers=headers, timeout=15, verify=False)
                    # å…³é”®å¢å¼ºï¼šæ£€æŸ¥èµ„æºæ˜¯å¦æˆåŠŸåŠ è½½
                    if response.status_code == 200:
                        self._proxy_raw_content(response)
                    else:
                        # èµ„æºåŠ è½½å¤±è´¥æ—¶è¿”å›ç©ºå†…å®¹è€Œä¸æ˜¯404ï¼Œé¿å…é˜»å¡é¡µé¢æ¸²æŸ“
                        self._send_empty_response()
                    return
                except requests.exceptions.RequestException:
                    # ç½‘ç»œé”™è¯¯æ—¶è¿”å›ç©ºå“åº”
                    self._send_empty_response()
                    return

        # å¦‚æœæ˜¯ç›´æ¥è®¿é—®æ ¹è·¯å¾„ï¼Œè¿”å›ä¸»é¡µ
        if self.path == '/':
            self._serve_homepage()
            return
            
        # æ— æ³•æ‰¾åˆ°èµ„æºæ—¶è¿”å›ç©ºå“åº”è€Œä¸æ˜¯404
        self._send_empty_response()

    def _send_empty_response(self):
        """å‘é€ç©ºå“åº”ï¼Œç”¨äºèµ„æºåŠ è½½å¤±è´¥æ—¶"""
        self.send_response(200)
        self.send_header('Content-Type', 'application/octet-stream')
        self.send_header('Content-Length', '0')
        self.end_headers()

    def _proxy_post_request_fixed(self):
        """ä¿®å¤çš„POSTè¯·æ±‚å¤„ç† - ä½¿ç”¨æ—§ç‰ˆæœ¬çš„å·¥ä½œé€»è¾‘"""
        query_params = urllib.parse.parse_qs(urllib.parse.urlparse(self.path).query)
        target_url = query_params.get('url', [''])[0]
        
        if not target_url:
            self.send_error(400, "Missing URL parameter")
            return
        
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length) if content_length > 0 else b''
        
        headers = {
            'User-Agent': self.config['user_agent'],
            'Content-Type': self.headers.get('Content-Type', 'application/x-www-form-urlencoded'),
        }
        
        try:
            response = requests.post(target_url, data=post_data, headers=headers, 
                                   timeout=30, verify=False, allow_redirects=False)
            
            print(f"POSTå“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code in [301, 302, 303, 307, 308]:
                location = response.headers.get('Location', '')
                if location:
                    print(f"æ‹¦æˆªåˆ°é‡å®šå‘: {location}")
                    
                    if not location.startswith(('http://', 'https://')):
                        location = urllib.parse.urljoin(target_url, location)
                    
                    proxy_location = "/proxy?url=" + urllib.parse.quote(location)
                    print(f"é‡å†™ä¸ºä»£ç†é‡å®šå‘: {proxy_location}")
                    
                    self.send_response(302)
                    self.send_header('Location', proxy_location)
                    self.end_headers()
                    return
            
            self._proxy_raw_content(response)
            
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to POST to target website: " + str(e))

    def _proxy_raw_content(self, response):
        """ä»£ç†åŸå§‹å†…å®¹"""
        self.send_response(response.status_code)
        
        # è¿‡æ»¤æ‰å¯èƒ½å¼•èµ·é—®é¢˜çš„å¤´éƒ¨
        excluded_headers = ['content-encoding', 'transfer-encoding', 'content-length', 'connection']
        for header, value in response.headers.items():
            if header.lower() not in excluded_headers:
                self.send_header(header, value)
        
        self.send_header('Content-Length', str(len(response.content)))
        self.end_headers()
        self.wfile.write(response.content)

    def _rewrite_html_content_complete(self, html_content, base_url):
        """å®Œæ•´çš„HTMLå†…å®¹é‡å†™ - åŒæ—¶ä¿®å¤æ–°é—»è·³è½¬å’Œæœç´¢"""
        soup = BeautifulSoup(html_content, 'html.parser')

        nav_html = '''
        <div style="background: #007cba; color: white; padding: 10px; margin: 0; text-align: center; position: fixed; top: 0; left: 0; right: 0; z-index: 10000;">
            <a href="/" style="color: white; text-decoration: none; font-weight: bold; margin-right: 15px;">ğŸ  è¿”å›ä»£ç†ä¸»é¡µ</a>
            <span>å½“å‰ä»£ç†: {}</span>
        </div>
        '''.format(base_url[:50] + '...' if len(base_url) > 50 else base_url)

        # å¢å¼ºçš„é“¾æ¥é‡å†™ - ä¿®å¤æ–°é—»è·³è½¬ï¼Œä½†ä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥
        self._rewrite_links_complete(soup, base_url)

        # é‡å†™CSSä¸­çš„url()å¼•ç”¨
        style_tags = soup.find_all('style')
        for style_tag in style_tags:
            if style_tag.string:
                style_tag.string = self._rewrite_css_urls(style_tag.string, base_url)

        # æ’å…¥å¯¼èˆªæ å¹¶æ·»åŠ é¡¶éƒ¨è¾¹è·
        body_tag = soup.find('body')
        if body_tag:
            # ä¸ºbodyæ·»åŠ é¡¶éƒ¨è¾¹è·ä»¥å®¹çº³å›ºå®šå¯¼èˆªæ 
            body_style = body_tag.get('style', '')
            if 'margin-top' not in body_style:
                body_tag['style'] = body_style + '; margin-top: 50px;' if body_style else 'margin-top: 50px;'
            
            nav_soup = BeautifulSoup(nav_html, 'html.parser')
            body_tag.insert(0, nav_soup.div)
        else:
            # å¦‚æœæ²¡æœ‰bodyæ ‡ç­¾ï¼Œåˆ›å»ºä¸€ä¸ªå¹¶æ’å…¥å¯¼èˆªæ 
            body_tag = soup.new_tag('body')
            body_tag['style'] = 'margin-top: 50px;'
            nav_soup = BeautifulSoup(nav_html, 'html.parser')
            body_tag.append(nav_soup.div)
            
            # å°†åŸæœ‰å†…å®¹ç§»åŠ¨åˆ°bodyä¸­
            for content in soup.contents:
                if content.name != 'body':
                    body_tag.append(content)
            soup.append(body_tag)

        # æ³¨å…¥å¢å¼ºçš„JavaScriptæ‹¦æˆªä»£ç  - å…³é”®ä¿®å¤æ–°é—»è·³è½¬
        interception_script = self._get_enhanced_interception_script(base_url)
        script_tag = soup.new_tag('script')
        script_tag.string = interception_script
        if body_tag:
            body_tag.append(script_tag)
        else:
            soup.append(script_tag)

        return str(soup)

    def _rewrite_links_complete(self, soup, base_url):
        """å®Œæ•´çš„é“¾æ¥é‡å†™ - ä¿®å¤æ–°é—»è·³è½¬ï¼Œä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥"""
        # é‡å†™æ™®é€šé“¾æ¥ - è·³è¿‡è¿”å›ä¸»é¡µé“¾æ¥
        for tag in soup.find_all('a'):
            if tag.get('href'):
                href = tag['href']
                # ç‰¹åˆ«ä¿æŠ¤è¿”å›ä¸»é¡µçš„é“¾æ¥
                if href == '/' or href.startswith('/?'):
                    continue
                if self._should_rewrite_url(href):
                    absolute_url = urllib.parse.urljoin(base_url, href)
                    tag['href'] = "/proxy?url=" + urllib.parse.quote(absolute_url)

        # é‡å†™è¡¨å•
        for form in soup.find_all('form'):
            if form.get('action'):
                action = form['action']
                if self._should_rewrite_url(action):
                    absolute_url = urllib.parse.urljoin(base_url, action)
                    form['action'] = "/proxy?url=" + urllib.parse.quote(absolute_url)

        # é‡å†™èµ„æº
        for tag in soup.find_all(['img', 'script', 'link', 'iframe']):
            src_attr = 'src' if tag.get('src') else 'href' if tag.get('href') else None
            if src_attr:
                src = tag[src_attr]
                if self._should_rewrite_url(src):
                    absolute_url = urllib.parse.urljoin(base_url, src)
                    tag[src_attr] = "/proxy?url=" + urllib.parse.quote(absolute_url)

        # é‡å†™meta refresh
        for meta in soup.find_all('meta', attrs={'http-equiv': re.compile('refresh', re.I)}):
            content = meta.get('content', '')
            if 'url=' in content.lower():
                parts = content.split(';', 1)
                if len(parts) == 2:
                    timeout, url_part = parts
                    if url_part.strip().lower().startswith('url='):
                        original_url = url_part[4:].strip()
                        if self._should_rewrite_url(original_url):
                            absolute_url = urllib.parse.urljoin(base_url, original_url)
                            proxy_url = "/proxy?url=" + urllib.parse.quote(absolute_url)
                            meta['content'] = f"{timeout}; URL={proxy_url}"

    def _should_rewrite_url(self, url):
        """åˆ¤æ–­URLæ˜¯å¦éœ€è¦é‡å†™"""
        if not url or url.strip() == '':
            return False
        if url.startswith(('#', 'javascript:', 'mailto:', 'tel:', 'data:', 'blob:')):
            return False
        if url.startswith(('/proxy?url=', '/resource/', 'http://localhost:', 'https://localhost:')):
            return False
        if url == '/' or url.startswith('/?'):
            return False
        return True

    def _rewrite_css_urls(self, css_content, base_url):
        """é‡å†™CSSä¸­çš„url()å¼•ç”¨"""
        import re

        def replace_url(match):
            url_content = match.group(1)
            if url_content.startswith(('http://', 'https://', 'data:')):
                return match.group(0)

            absolute_url = urllib.parse.urljoin(base_url, url_content.strip('"\''))
            proxy_url = "/proxy?url=" + urllib.parse.quote(absolute_url)
            return 'url("' + proxy_url + '")'

        pattern = r'url\(["\']?([^"\'()]*)["\']?\)'
        return re.sub(pattern, replace_url, css_content)

    def _get_enhanced_interception_script(self, base_url):
        """è·å–å¢å¼ºçš„JavaScriptæ‹¦æˆªä»£ç  - å½»åº•ä¿®å¤æ–°é—»è·³è½¬"""
        return '''
        // å¢å¼ºçš„JavaScriptæ‹¦æˆªä»£ç  - ä¸“é—¨ä¿®å¤æ–°é—»è·³è½¬
        (function() {
            'use strict';
            
            var baseUrl = "''' + base_url + '''";
            
            // 1. å¢å¼ºçš„ç‚¹å‡»äº‹ä»¶æ‹¦æˆª - å¤„ç†æ‰€æœ‰å¯èƒ½çš„æ–°é—»é“¾æ¥
            function interceptClickEvent(e) {
                var target = e.target;
                
                // å‘ä¸Šéå†æ‰€æœ‰çˆ¶å…ƒç´ ï¼ŒæŸ¥æ‰¾é“¾æ¥
                while (target && target !== document) {
                    if (target.tagName && target.tagName.toLowerCase() === 'a' && target.href) {
                        var href = target.href;
                        
                        // æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦ä»£ç†çš„å¤–éƒ¨é“¾æ¥ï¼ˆç‰¹åˆ«ä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥ï¼‰
                        if (href && 
                            !href.includes('/proxy?url=') && 
                            !href.startsWith('javascript:') && 
                            !href.startsWith('mailto:') && 
                            !href.startsWith('tel:') && 
                            !href.startsWith('#') &&
                            !href.startsWith('data:') &&
                            !(target.getAttribute('href') === '/') && // ä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥
                            !(target.textContent && target.textContent.includes('è¿”å›ä»£ç†ä¸»é¡µ'))) {
                            
                            e.preventDefault();
                            e.stopImmediatePropagation();
                            e.stopPropagation();
                            
                            // è½¬æ¢ä¸ºä»£ç†URL
                            try {
                                var fullUrl = new URL(href, baseUrl).href;
                                var proxyUrl = '/proxy?url=' + encodeURIComponent(fullUrl);
                                window.location.href = proxyUrl;
                            } catch (err) {
                                console.log('æ‹¦æˆªé”™è¯¯:', err);
                            }
                            return false;
                        }
                    }
                    target = target.parentNode;
                }
            }
            
            // 2. å¤šå±‚çº§äº‹ä»¶ç›‘å¬ - ç¡®ä¿æ•è·æ‰€æœ‰ç‚¹å‡»
            document.addEventListener('click', interceptClickEvent, true);
            document.addEventListener('auxclick', interceptClickEvent, true); // ä¸­é”®ç‚¹å‡»
            document.addEventListener('contextmenu', interceptClickEvent, true); // å³é”®èœå•
            
            // 3. æ‹¦æˆªåŠ¨æ€åˆ›å»ºçš„é“¾æ¥
            var observer = new MutationObserver(function(mutations) {
                mutations.forEach(function(mutation) {
                    mutation.addedNodes.forEach(function(node) {
                        if (node.nodeType === 1) { // Element node
                            // æ£€æŸ¥æ–°å¢çš„é“¾æ¥
                            if (node.tagName === 'A' && node.href && !node.href.includes('/proxy?url=')) {
                                var fullUrl = new URL(node.href, baseUrl).href;
                                node.href = '/proxy?url=' + encodeURIComponent(fullUrl);
                            }
                            
                            // æ£€æŸ¥å­å…ƒç´ ä¸­çš„é“¾æ¥
                            var links = node.querySelectorAll ? node.querySelectorAll('a') : [];
                            for (var i = 0; i < links.length; i++) {
                                var link = links[i];
                                if (link.href && !link.href.includes('/proxy?url=')) {
                                    var fullUrl = new URL(link.href, baseUrl).href;
                                    link.href = '/proxy?url=' + encodeURIComponent(fullUrl);
                                }
                            }
                        }
                    });
                });
            });
            
            // 4. å¼€å§‹è§‚å¯ŸDOMå˜åŒ–
            observer.observe(document.body, {
                childList: true,
                subtree: true
            });
            
            // 5. æ‹¦æˆªwindow.openç­‰æ–¹æ³•
            var originalWindowOpen = window.open;
            window.open = function(url, target, features) {
                if (url && !url.includes('/proxy?url=')) {
                    var fullUrl = new URL(url, baseUrl).href;
                    url = '/proxy?url=' + encodeURIComponent(fullUrl);
                }
                return originalWindowOpen.call(this, url, target, features);
            };
            
            console.log('ğŸ”’ å¢å¼ºæ‹¦æˆªè„šæœ¬å·²åŠ è½½ - æ–°é—»è·³è½¬å·²ä¿®å¤');
        })();
        '''

    def log_message(self, format, *args):
        """è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼"""
        print("[" + self.log_date_time_string() + "] " + format % args)

def run_proxy_server():
    """è¿è¡Œä»£ç†æœåŠ¡å™¨"""
    port = 60000
    
    try:
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    except:
        pass
    
    with socketserver.TCPServer(("", port), CompleteFixProxyHandler) as httpd:
        print("ğŸš€ å®Œæ•´ä¿®å¤ä»£ç†æœåŠ¡å™¨å·²å¯åŠ¨åœ¨ç«¯å£ " + str(port))
        print("ğŸ“§ è®¿é—®åœ°å€: http://localhost:" + str(port))
        print("ğŸ”§ ä¿®å¤å†…å®¹: æ–°é—»ç›´æ¥è·³è½¬ + æœç´¢é—®é¢˜ + ä¸»é¡µå¾ªç¯")
        print("â¹ï¸ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nğŸ›‘ æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    run_proxy_server()
