#!/usr/bin/env python3
"""
HTTPä»£ç†æœåŠ¡å™¨ - 360æœç´¢ä¹±ç ä¿®å¤ç‰ˆ
è¿è¡Œç«¯å£: 60000
ä¸“é—¨ä¿®å¤360æœç´¢ä¹±ç é—®é¢˜
"""

import http.server
import socketserver
import urllib.parse
import requests
from bsup4 import BeautifulSoup
import re
import time
import chardet

class SoComFixProxyHandler(http.server.BaseHTTPRequestHandler):
    """ä¸“é—¨ä¿®å¤360æœç´¢ä¹±ç çš„ä»£ç†å¤„ç†å™¨"""

    config = {
        'port': 60000,
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    def do_GET(self):
        """å¤„ç†GETè¯·æ±‚"""
        try:
            print(f"GETè¯·æ±‚: {self.path}")
            
            if self.path == '/':
                self._serve_homepage()
            elif self.path.startswith('/proxy?url='):
                self._proxy_webpage_enhanced()
            elif self.path.startswith('/proxy?') and 'url=' not in self.path:
                # å…³é”®ä¿®å¤ï¼šå¤„ç†æœç´¢åç”Ÿæˆçš„URLï¼ˆæ²¡æœ‰urlå‚æ•°çš„æƒ…å†µï¼‰
                self._handle_search_result()
            else:
                self._proxy_resource()
        except Exception as e:
            self.send_error(500, "Server Error: " + str(e))

    def do_POST(self):
        """å¤„ç†POSTè¯·æ±‚"""
        try:
            if self.path.startswith('/proxy?url='):
                self._proxy_post_request_enhanced()
            else:
                self.send_error(404, "Not Found")
        except Exception as e:
            self.send_error(500, "Server Error: " + str(e))

    def _handle_search_result(self):
        """ä¸“é—¨å¤„ç†æœç´¢åç”Ÿæˆçš„URL - å…³é”®ä¿®å¤æœç´¢é—®é¢˜"""
        try:
            # è§£æå½“å‰è·¯å¾„ä¸­çš„å‚æ•°
            parsed_path = urllib.parse.urlparse(self.path)
            query_params = urllib.parse.parse_qs(parsed_path.query)
            
            print(f"æœç´¢å‚æ•°: {query_params}")
            
            # ä»Refererä¸­è·å–åŸå§‹æœç´¢é¡µé¢URL
            referer = self.headers.get('Referer', '')
            if referer and '/proxy?url=' in referer:
                # æå–åŸå§‹æœç´¢é¡µé¢çš„URL
                referer_parsed = urllib.parse.urlparse(referer)
                referer_query = urllib.parse.parse_qs(referer_parsed.query)
                base_search_url = referer_query.get('url', [''])[0]
                
                if base_search_url:
                    # æ„å»ºæœç´¢ç»“æœçš„å®Œæ•´URL
                    # å¯¹äº360æœç´¢ï¼Œæœç´¢ç»“æœé¡µé€šå¸¸æ˜¯ www.so.com/s åŠ ä¸Šå‚æ•°
                    search_result_url = "https://www.so.com/s"
                    
                    # æ·»åŠ æ‰€æœ‰æœç´¢å‚æ•°
                    if query_params:
                        search_result_url += "?" + urllib.parse.urlencode(query_params, doseq=True)
                    
                    print(f"æ„å»ºæœç´¢ç»“æœURL: {search_result_url}")
                    
                    # ä»£ç†è¿™ä¸ªæœç´¢ç»“æœé¡µé¢
                    self._proxy_specific_url(search_result_url)
                    return
            
            # å¦‚æœæ— æ³•ä»Refererè·å–ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            self._try_auto_fix_search()
            
        except Exception as e:
            print(f"æœç´¢å¤„ç†é”™è¯¯: {str(e)}")
            self.send_error(500, "Search processing error: " + str(e))

    def _try_auto_fix_search(self):
        """å°è¯•è‡ªåŠ¨ä¿®å¤æœç´¢URL"""
        try:
            # è§£æå½“å‰è·¯å¾„
            parsed_path = urllib.parse.urlparse(self.path)
            query_params = urllib.parse.parse_qs(parsed_path.query)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœç´¢å‚æ•°
            if 'q' in query_params or 'query' in query_params or 'keyword' in query_params:
                # å‡è®¾æ˜¯360æœç´¢
                search_result_url = "https://www.so.com/s"
                
                if query_params:
                    search_result_url += "?" + urllib.parse.urlencode(query_params, doseq=True)
                
                print(f"è‡ªåŠ¨ä¿®å¤æœç´¢URL: {search_result_url}")
                self._proxy_specific_url(search_result_url)
                return
            
            # å¦‚æœæ— æ³•ä¿®å¤ï¼Œè¿”å›é”™è¯¯
            self.send_error(400, "æ— æ³•å¤„ç†çš„æœç´¢è¯·æ±‚")
            
        except Exception as e:
            print(f"è‡ªåŠ¨ä¿®å¤é”™è¯¯: {str(e)}")
            self.send_error(500, "Auto-fix error: " + str(e))

    def _proxy_specific_url(self, target_url):
        """ä»£ç†ç‰¹å®šURL"""
        print(f"ä»£ç†ç‰¹å®šURL: {target_url}")
        
        headers = self._get_enhanced_headers(target_url)
        
        try:
            response = requests.get(target_url, headers=headers, timeout=30, verify=False)
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to fetch: " + str(e))
            return
        
        # å¤„ç†å„ç§å“åº”çŠ¶æ€ç 
        if response.status_code != 200:
            self._handle_non_200_response(response, target_url)
            return
        
        # ç‰¹æ®Šå¤„ç†360æœç´¢
        if 'so.com' in target_url:
            rewritten_content = self._rewrite_so_com_content(response, target_url)
        else:
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                rewritten_content = self._rewrite_html_content_with_encoding(response, target_url)
            else:
                self._proxy_raw_content(response)
                return
        
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.send_header('Content-Length', str(len(rewritten_content)))
        self.end_headers()
        self.wfile.write(rewritten_content)

    def _rewrite_so_com_content(self, response, base_url):
        """ä¸“é—¨é‡å†™360æœç´¢å†…å®¹ - ä¿®å¤ä¹±ç é—®é¢˜"""
        print("æ£€æµ‹åˆ°360æœç´¢é¡µé¢ï¼Œä½¿ç”¨ä¸“é—¨å¤„ç†")
        
        # 360æœç´¢é€šå¸¸ä½¿ç”¨GBKç¼–ç 
        try:
            # å…ˆå°è¯•GBKç¼–ç 
            html_content = response.content.decode('gbk', errors='replace')
            print("ä½¿ç”¨GBKç¼–ç è§£ç æˆåŠŸ")
        except UnicodeDecodeError:
            try:
                # å¦‚æœGBKå¤±è´¥ï¼Œå°è¯•GB2312
                html_content = response.content.decode('gb2312', errors='replace')
                print("ä½¿ç”¨GB2312ç¼–ç è§£ç æˆåŠŸ")
            except UnicodeDecodeError:
                try:
                    # å¦‚æœGB2312å¤±è´¥ï¼Œå°è¯•UTF-8
                    html_content = response.content.decode('utf-8', errors='replace')
                    print("ä½¿ç”¨UTF-8ç¼–ç è§£ç æˆåŠŸ")
                except UnicodeDecodeError:
                    # æœ€åä½¿ç”¨chardetæ£€æµ‹
                    detected = chardet.detect(response.content)
                    encoding = detected.get('encoding', 'utf-8')
                    print(f"ä½¿ç”¨chardetæ£€æµ‹ç¼–ç : {encoding}")
                    html_content = response.content.decode(encoding, errors='replace')
        
        # ä½¿ç”¨BeautifulSoupè§£æå¹¶é‡å†™
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
        except Exception as e:
            print(f"BeautifulSoupè§£æé”™è¯¯: {e}, ä½¿ç”¨å®½æ¾è§£æ")
            soup = BeautifulSoup(html_content, 'html.parser')
        
        # å¯¼èˆªæ HTML
        nav_html = '''
        <div style="background: #007cba; color: white; padding: 10px; margin: 0; text-align: center; position: fixed; top: 0; left: 0; right: 0; z-index: 10000;">
            <a href="/" style="color: white; text-decoration: none; font-weight: bold; margin-right: 15px;">ğŸ  è¿”å›ä»£ç†ä¸»é¡µ</a>
            <span>å½“å‰ä»£ç†: {}</span>
        </div>
        '''.format(base_url[:50] + '...' if len(base_url) > 50 else base_url)

        # é‡å†™é“¾æ¥
        self._rewrite_links_complete(soup, base_url)

        # é‡å†™CSSä¸­çš„url()å¼•ç”¨
        style_tags = soup.find_all('style')
        for style_tag in style_tags:
            if style_tag.string:
                style_tag.string = self._rewrite_css_urls(style_tag.string, base_url)

        # ç¡®ä¿æœ‰æ­£ç¡®çš„å­—ç¬¦é›†å£°æ˜
        head_tag = soup.find('head')
        if head_tag:
            # ç§»é™¤ç°æœ‰çš„charsetå£°æ˜
            for meta in head_tag.find_all('meta', attrs={'charset': True}):
                meta.decompose()
            for meta in head_tag.find_all('meta', attrs={'http-equiv': lambda x: x and x.lower() == 'content-type'}):
                meta.decompose()
            
            # æ·»åŠ UTF-8 charsetå£°æ˜
            new_meta = soup.new_tag('meta', charset='utf-8')
            head_tag.insert(0, new_meta)
        else:
            # å¦‚æœæ²¡æœ‰headæ ‡ç­¾ï¼Œåˆ›å»ºä¸€ä¸ª
            head_tag = soup.new_tag('head')
            soup.insert(0, head_tag)
            new_meta = soup.new_tag('meta', charset='utf-8')
            head_tag.insert(0, new_meta)

        # æ’å…¥å¯¼èˆªæ å¹¶æ·»åŠ é¡¶éƒ¨è¾¹è·
        body_tag = soup.find('body')
        if body_tag:
            # ä¸ºbodyæ·»åŠ é¡¶éƒ¨è¾¹è·ä»¥å®¹çº³å›ºå®šå¯¼èˆªæ 
            body_style = body_tag.get('style', '')
            if 'margin-top' not in body_style:
                body_tag['style'] = body_style + '; margin-top: 50px;' if body_style else 'margin-top: 50px;'
            
            nav_soup = BeautifulSoup(nav_html, 'html.parser')
            body_tag.insert(0, nav_soup.div)
        else:
            # å¦‚æœæ²¡æœ‰bodyæ ‡ç­¾ï¼Œåˆ›å»ºä¸€ä¸ªå¹¶æ’å…¥å¯¼èˆªæ 
            body_tag = soup.new_tag('body')
            body_tag['style'] = 'margin-top: 50px;'
            nav_soup = BeautifulSoup(nav_html, 'html.parser')
            body_tag.append(nav_soup.div)
            
            # å°†åŸæœ‰å†…å®¹ç§»åŠ¨åˆ°bodyä¸­
            for content in soup.contents:
                if content.name != 'body':
                    body_tag.append(content)
            soup.append(body_tag)

        # æ³¨å…¥å¢å¼ºçš„JavaScriptæ‹¦æˆªä»£ç 
        interception_script = self._get_enhanced_interception_script(base_url)
        script_tag = soup.new_tag('script')
        script_tag.string = interception_script
        if body_tag:
            body_tag.append(script_tag)
        else:
            soup.append(script_tag)

        # è¿”å›UTF-8ç¼–ç çš„å­—èŠ‚ä¸²
        return str(soup).encode('utf-8')

    def _get_enhanced_headers(self, target_url):
        """è·å–å¢å¼ºçš„è¯·æ±‚å¤´ - ä¿®å¤412é”™è¯¯"""
        headers = {
            'User-Agent': self.config['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
        }
        
        # ä¸ºç‰¹å®šç½‘ç«™æ·»åŠ Referer
        if 'bilibili.com' in target_url:
            headers['Referer'] = 'https://www.bilibili.com/'
        elif 'so.com' in target_url:
            headers['Referer'] = 'https://www.so.com/'
        else:
            headers['Referer'] = target_url
            
        return headers

    def _serve_homepage(self):
        """æä¾›ä»£ç†ä¸»é¡µç•Œé¢"""
        homepage_html = '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>360æœç´¢ä¹±ç ä¿®å¤ä»£ç† - 60000ç«¯å£</title>
            <meta charset="utf-8">
            <style>
                body { 
                    font-family: Arial, sans-serif; 
                    margin: 40px; 
                    background: #f5f5f5;
                }
                .container { 
                    max-width: 800px; 
                    margin: 0 auto; 
                    background: white; 
                    padding: 30px; 
                    border-radius: 10px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }
                h1 { 
                    color: #333; 
                    text-align: center;
                }
                .form-group { 
                    margin: 20px 0; 
                }
                input[type="url"] { 
                    width: 100%; 
                    padding: 12px; 
                    border: 1px solid #ddd; 
                    border-radius: 5px; 
                    font-size: 16px;
                    box-sizing: border-box;
                }
                button { 
                    background: #007cba; 
                    color: white; 
                    border: none; 
                    padding: 12px 24px; 
                    border-radius: 5px; 
                    cursor: pointer; 
                    font-size: 16px;
                    width: 100%;
                }
                button:hover { 
                    background: #005a87; 
                }
                .info {
                    background: #e7f3ff;
                    padding: 15px;
                    border-radius: 5px;
                    margin: 20px 0;
                    border-left: 4px solid #007cba;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸŒ 360æœç´¢ä¹±ç ä¿®å¤ä»£ç†</h1>
                <div class="info">
                    <strong>ä¿®å¤å†…å®¹ï¼š</strong> ä¸“é—¨è§£å†³360æœç´¢ä¹±ç é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–ä¿®å¤ã€‚
                </div>
                <form action="/proxy" method="GET">
                    <div class="form-group">
                        <input type="url" name="url" placeholder="https://www.example.com" required value="https://www.so.com">
                    </div>
                    <button type="submit">å¼€å§‹ä»£ç†è®¿é—®</button>
                </form>
                <div style="margin-top: 20px; text-align: center; color: #666;">
                    <small>ä»£ç†æœåŠ¡è¿è¡Œåœ¨ç«¯å£ 60000 | å·²ä¿®å¤360æœç´¢ä¹±ç é—®é¢˜</small>
                </div>
            </div>
        </body>
        </html>
        '''
        
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.send_header('Content-Length', str(len(homepage_html.encode('utf-8'))))
        self.end_headers()
        self.wfile.write(homepage_html.encode('utf-8'))

    def _proxy_webpage_enhanced(self):
        """å¢å¼ºçš„ç½‘é¡µä»£ç† - ä¿®å¤ä¹±ç ã€ä¸‹è½½æ–‡ä»¶å’Œ412é”™è¯¯"""
        query_params = urllib.parse.parse_qs(urllib.parse.urlparse(self.path).query)
        target_url = query_params.get('url', [''])[0]

        if not target_url:
            self.send_error(400, "Missing URL parameter")
            return

        if not target_url.startswith(('http://', 'https://')):
            target_url = 'http://' + target_url

        headers = self._get_enhanced_headers(target_url)

        try:
            response = requests.get(target_url, headers=headers, timeout=30, verify=False)
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to fetch target website: " + str(e))
            return

        # å¤„ç†é200çŠ¶æ€ç  - ä¿®å¤ä¸‹è½½æ–‡ä»¶é—®é¢˜
        if response.status_code != 200:
            self._handle_non_200_response(response, target_url)
            return

        # ç‰¹æ®Šå¤„ç†360æœç´¢
        if 'so.com' in target_url:
            rewritten_content = self._rewrite_so_com_content(response, target_url)
        else:
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                rewritten_content = self._rewrite_html_content_with_encoding(response, target_url)
            else:
                self._proxy_raw_content(response)
                return
        
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.send_header('Content-Length', str(len(rewritten_content)))
        self.end_headers()
        self.wfile.write(rewritten_content)

    def _rewrite_html_content_with_encoding(self, response, base_url):
        """å¸¦ç¼–ç å¤„ç†çš„HTMLå†…å®¹é‡å†™ - ä¿®å¤ä¹±ç é—®é¢˜"""
        # æ£€æµ‹åŸå§‹ç¼–ç 
        original_encoding = self._detect_encoding(response)
        print(f"æ£€æµ‹åˆ°ç¼–ç : {original_encoding}")
        
        try:
            # ä½¿ç”¨æ­£ç¡®ç¼–ç è§£ç å†…å®¹
            if original_encoding:
                html_content = response.content.decode(original_encoding, errors='replace')
            else:
                # å¦‚æœæ— æ³•æ£€æµ‹ç¼–ç ï¼Œå°è¯•å¸¸è§ç¼–ç 
                for encoding in ['utf-8', 'gbk', 'gb2312', 'iso-8859-1']:
                    try:
                        html_content = response.content.decode(encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨replaceæ¨¡å¼
                    html_content = response.content.decode('utf-8', errors='replace')
        except Exception as e:
            print(f"è§£ç é”™è¯¯: {e}, ä½¿ç”¨replaceæ¨¡å¼")
            html_content = response.content.decode('utf-8', errors='replace')
        
        return self._rewrite_html_content_complete(html_content, base_url)

    def _detect_encoding(self, response):
        """æ£€æµ‹å“åº”å†…å®¹çš„ç¼–ç """
        # é¦–å…ˆæ£€æŸ¥Content-Typeå¤´ä¸­çš„ç¼–ç 
        content_type = response.headers.get('Content-Type', '').lower()
        if 'charset=' in content_type:
            charset_match = re.search(r'charset=([^\s;]+)', content_type)
            if charset_match:
                encoding = charset_match.group(1)
                print(f"ä»Content-Typeæ£€æµ‹åˆ°ç¼–ç : {encoding}")
                return encoding
        
        # æ£€æŸ¥HTML metaæ ‡ç­¾ä¸­çš„ç¼–ç 
        try:
            # åªæ£€æŸ¥å‰2000å­—èŠ‚æ¥æ£€æµ‹metaæ ‡ç­¾
            content_preview = response.content[:2000]
            soup = BeautifulSoup(content_preview, 'html.parser')
            meta_charset = soup.find('meta', attrs={'charset': True})
            if meta_charset:
                encoding = meta_charset.get('charset')
                print(f"ä»meta charsetæ£€æµ‹åˆ°ç¼–ç : {encoding}")
                return encoding
            
            meta_content_type = soup.find('meta', attrs={'http-equiv': re.compile('content-type', re.I)})
            if meta_content_type and meta_content_type.get('content'):
                content_value = meta_content_type.get('content')
                if 'charset=' in content_value.lower():
                    charset_match = re.search(r'charset=([^\s;]+)', content_value, re.I)
                    if charset_match:
                        encoding = charset_match.group(1)
                        print(f"ä»meta http-equivæ£€æµ‹åˆ°ç¼–ç : {encoding}")
                        return encoding
        except Exception as e:
            print(f"è§£æmetaæ ‡ç­¾é”™è¯¯: {e}")
        
        # ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
        try:
            detected = chardet.detect(response.content)
            if detected['confidence'] > 0.7:
                encoding = detected['encoding']
                print(f"chardetæ£€æµ‹åˆ°ç¼–ç : {encoding} (ç½®ä¿¡åº¦: {detected['confidence']})")
                return encoding
        except Exception as e:
            print(f"chardetæ£€æµ‹é”™è¯¯: {e}")
        
        print("æ— æ³•æ£€æµ‹ç¼–ç ï¼Œä½¿ç”¨utf-8")
        return 'utf-8'

    def _handle_non_200_response(self, response, target_url):
        """å¤„ç†é200çŠ¶æ€ç çš„å“åº” - ä¿®å¤ä¸‹è½½æ–‡ä»¶é—®é¢˜"""
        status_code = response.status_code
        
        # å¤„ç†é‡å®šå‘
        if status_code in [301, 302, 303, 307, 308]:
            location = response.headers.get('Location', '')
            if location:
                if not location.startswith(('http://', 'https://')):
                    location = urllib.parse.urljoin(target_url, location)
                
                proxy_location = "/proxy?url=" + urllib.parse.quote(location)
                
                # è¿”å›HTMLé‡å®šå‘é¡µé¢è€Œä¸æ˜¯ç›´æ¥é‡å®šå‘
                redirect_html = f'''
                <!DOCTYPE html>
                <html>
                <head>
                    <title>é‡å®šå‘ä¸­</title>
                    <meta http-equiv="refresh" content="0;url={proxy_location}">
                    <meta charset="utf-8">
                </head>
                <body>
                    <p>æ­£åœ¨é‡å®šå‘... <a href="{proxy_location}">ç‚¹å‡»è¿™é‡Œ</a> å¦‚æœé¡µé¢æ²¡æœ‰è‡ªåŠ¨è·³è½¬ã€‚</p>
                </body>
                </html>
                '''
                
                self.send_response(200)
                self.send_header('Content-Type', 'text/html; charset=utf-8')
                self.send_header('Content-Length', str(len(redirect_html.encode('utf-8'))))
                self.end_headers()
                self.wfile.write(redirect_html.encode('utf-8'))
                return
        
        # å¤„ç†412é”™è¯¯å’Œå…¶ä»–é”™è¯¯
        error_html = self._generate_error_html(status_code, response.reason, target_url)
        self.send_response(200)  # æ€»æ˜¯è¿”å›200ï¼Œé¿å…æµè§ˆå™¨ä¸‹è½½é”™è¯¯å“åº”
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.send_header('Content-Length', str(len(error_html.encode('utf-8'))))
        self.end_headers()
        self.wfile.write(error_html.encode('utf-8'))

    def _generate_error_html(self, status_code, reason, target_url):
        """ç”Ÿæˆé”™è¯¯é¡µé¢HTML"""
        if status_code == 412:
            return f'''
            <!DOCTYPE html>
            <html>
            <head>
                <title>412é”™è¯¯ - ä»£ç†è®¿é—®è¢«æ‹’ç»</title>
                <meta charset="utf-8">
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 40px; }}
                    .error-container {{ max-width: 600px; margin: 0 auto; padding: 20px; border: 1px solid #e0e0e0; border-radius: 5px; }}
                    .error-code {{ color: #d32f2f; font-size: 24px; font-weight: bold; }}
                    .suggestion {{ background: #fff3e0; padding: 15px; border-radius: 5px; margin: 20px 0; }}
                </style>
            </head>
            <body>
                <div class="error-container">
                    <h1>ä»£ç†è®¿é—®é‡åˆ°é—®é¢˜</h1>
                    <div class="error-code">é”™è¯¯ä»£ç : {status_code} - {reason}</div>
                    <p>ç›®æ ‡ç½‘ç«™æ‹’ç»äº†æ­¤è¯·æ±‚ã€‚å¯¹äºBç«™ç­‰ç½‘ç«™ï¼Œè¿™é€šå¸¸æ˜¯ç”±äºåçˆ¬è™«æœºåˆ¶ã€‚</p>
                    
                    <div class="suggestion">
                        <h3>å»ºè®®è§£å†³æ–¹æ¡ˆ:</h3>
                        <ul>
                            <li>åˆ·æ–°é¡µé¢é‡è¯•</li>
                            <li>æ£€æŸ¥ä»£ç†æœåŠ¡å™¨è¯·æ±‚å¤´è®¾ç½®</li>
                            <li>ç¡®è®¤ç›®æ ‡ç½‘ç«™å¯æ­£å¸¸è®¿é—®</li>
                        </ul>
                    </div>
                    
                    <p><a href="/proxy?url={urllib.parse.quote(target_url)}">é‡æ–°å°è¯•è®¿é—®æ­¤é¡µé¢</a></p>
                    <p><a href="/">è¿”å›ä»£ç†ä¸»é¡µ</a></p>
                </div>
            </body>
            </html>
            '''
        else:
            return f'''
            <!DOCTYPE html>
            <html>
            <head>
                <title>{status_code}é”™è¯¯</title>
                <meta charset="utf-8">
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 40px; }}
                    .error-container {{ max-width: 600px; margin: 0 auto; padding: 20px; border: 1px solid #e0e0e0; border-radius: 5px; }}
                    .error-code {{ color: #d32f2f; font-size: 24px; font-weight: bold; }}
                </style>
            </head>
            <body>
                <div class="error-container">
                    <h1>ä»£ç†è®¿é—®é‡åˆ°é—®é¢˜</h1>
                    <div class="error-code">é”™è¯¯ä»£ç : {status_code} - {reason}</div>
                    <p>ç›®æ ‡ç½‘ç«™è¿”å›äº†é”™è¯¯å“åº”ã€‚</p>
                    
                    <p><a href="/proxy?url={urllib.parse.quote(target_url)}">é‡æ–°å°è¯•è®¿é—®æ­¤é¡µé¢</a></p>
                    <p><a href="/">è¿”å›ä»£ç†ä¸»é¡µ</a></p>
                </div>
            </body>
            </html>
            '''

    def _proxy_resource(self):
        """ä»£ç†é™æ€èµ„æº - å¢å¼ºé”™è¯¯å¤„ç†"""
        referer = self.headers.get('Referer', '')

        if '/proxy?url=' in referer:
            referer_parsed = urllib.parse.urlparse(referer)
            referer_query = urllib.parse.parse_qs(referer_parsed.query)
            base_url = referer_query.get('url', [''])[0]

            if base_url:
                # å¤„ç†è¿”å›ä¸»é¡µçš„ç‰¹æ®Šæƒ…å†µ
                if self.path == '/':
                    self._serve_homepage()
                    return
                    
                resource_url = urllib.parse.urljoin(base_url, self.path)

                headers = {
                    'User-Agent': self.config['user_agent'],
                    'Referer': base_url
                }

                try:
                    response = requests.get(resource_url, headers=headers, timeout=15, verify=False)
                    # å…³é”®å¢å¼ºï¼šæ£€æŸ¥èµ„æºæ˜¯å¦æˆåŠŸåŠ è½½
                    if response.status_code == 200:
                        self._proxy_raw_content(response)
                    else:
                        # èµ„æºåŠ è½½å¤±è´¥æ—¶è¿”å›ç©ºå†…å®¹è€Œä¸æ˜¯404ï¼Œé¿å…é˜»å¡é¡µé¢æ¸²æŸ“
                        self._send_empty_response()
                    return
                except requests.exceptions.RequestException:
                    # ç½‘ç»œé”™è¯¯æ—¶è¿”å›ç©ºå“åº”
                    self._send_empty_response()
                    return

        # å¦‚æœæ˜¯ç›´æ¥è®¿é—®æ ¹è·¯å¾„ï¼Œè¿”å›ä¸»é¡µ
        if self.path == '/':
            self._serve_homepage()
            return
            
        # æ— æ³•æ‰¾åˆ°èµ„æºæ—¶è¿”å›ç©ºå“åº”è€Œä¸æ˜¯404
        self._send_empty_response()

    def _send_empty_response(self):
        """å‘é€ç©ºå“åº”ï¼Œç”¨äºèµ„æºåŠ è½½å¤±è´¥æ—¶"""
        self.send_response(200)
        self.send_header('Content-Type', 'application/octet-stream')
        self.send_header('Content-Length', '0')
        self.end_headers()

    def _proxy_post_request_enhanced(self):
        """å¢å¼ºçš„POSTè¯·æ±‚å¤„ç† - ä¿®å¤ä¸‹è½½æ–‡ä»¶é—®é¢˜"""
        query_params = urllib.parse.parse_qs(urllib.parse.urlparse(self.path).query)
        target_url = query_params.get('url', [''])[0]
        
        if not target_url:
            self.send_error(400, "Missing URL parameter")
            return
        
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length) if content_length > 0 else b''
        
        headers = self._get_enhanced_headers(target_url)
        headers['Content-Type'] = self.headers.get('Content-Type', 'application/x-www-form-urlencoded')
        
        try:
            response = requests.post(target_url, data=post_data, headers=headers, 
                                   timeout=30, verify=False, allow_redirects=False)
            
            print(f"POSTå“åº”çŠ¶æ€ç : {response.status_code}")
            
            # å¤„ç†é‡å®šå‘
            if response.status_code in [301, 302, 303, 307, 308]:
                location = response.headers.get('Location', '')
                if location:
                    print(f"æ‹¦æˆªåˆ°é‡å®šå‘: {location}")
                    
                    if not location.startswith(('http://', 'https://')):
                        location = urllib.parse.urljoin(target_url, location)
                    
                    proxy_location = "/proxy?url=" + urllib.parse.quote(location)
                    print(f"é‡å†™ä¸ºä»£ç†é‡å®šå‘: {proxy_location}")
                    
                    # è¿”å›HTMLé‡å®šå‘é¡µé¢è€Œä¸æ˜¯ç›´æ¥é‡å®šå‘
                    redirect_html = f'''
                    <!DOCTYPE html>
                    <html>
                    <head>
                        <title>é‡å®šå‘ä¸­</title>
                        <meta http-equiv="refresh" content="0;url={proxy_location}">
                        <meta charset="utf-8">
                    </head>
                    <body>
                        <p>æ­£åœ¨é‡å®šå‘... <a href="{proxy_location}">ç‚¹å‡»è¿™é‡Œ</a> å¦‚æœé¡µé¢æ²¡æœ‰è‡ªåŠ¨è·³è½¬ã€‚</p>
                    </body>
                    </html>
                    '''
                    
                    self.send_response(200)
                    self.send_header('Content-Type', 'text/html; charset=utf-8')
                    self.send_header('Content-Length', str(len(redirect_html.encode('utf-8'))))
                    self.end_headers()
                    self.wfile.write(redirect_html.encode('utf-8'))
                    return
            
            # å¤„ç†é200çŠ¶æ€ç 
            if response.status_code != 200:
                self._handle_non_200_response(response, target_url)
                return
                
            # æ­£å¸¸å¤„ç†200å“åº”
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                # ç‰¹æ®Šå¤„ç†360æœç´¢
                if 'so.com' in target_url:
                    rewritten_content = self._rewrite_so_com_content(response, target_url)
                else:
                    rewritten_content = self._rewrite_html_content_with_encoding(response, target_url)
                self.send_response(200)
                self.send_header('Content-Type', 'text/html; charset=utf-8')
                self.send_header('Content-Length', str(len(rewritten_content)))
                self.end_headers()
                self.wfile.write(rewritten_content)
            else:
                self._proxy_raw_content(response)
            
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to POST to target website: " + str(e))

    def _proxy_raw_content(self, response):
        """ä»£ç†åŸå§‹å†…å®¹"""
        self.send_response(response.status_code)
        
        # è¿‡æ»¤æ‰å¯èƒ½å¼•èµ·é—®é¢˜çš„å¤´éƒ¨
        excluded_headers = ['content-encoding', 'transfer-encoding', 'content-length', 'connection']
        for header, value in response.headers.items():
            if header.lower() not in excluded_headers:
                self.send_header(header, value)
        
        self.send_header('Content-Length', str(len(response.content)))
        self.end_headers()
        self.wfile.write(response.content)

    def _rewrite_html_content_complete(self, html_content, base_url):
        """å®Œæ•´çš„HTMLå†…å®¹é‡å†™ - åŒæ—¶ä¿®å¤æ–°é—»è·³è½¬å’Œæœç´¢"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
        except Exception as e:
            print(f"BeautifulSoupè§£æé”™è¯¯: {e}, ä½¿ç”¨å®½æ¾è§£æ")
            soup = BeautifulSoup(html_content, 'html.parser')

        nav_html = '''
        <div style="background: #007cba; color: white; padding: 10px; margin: 0; text-align: center; position: fixed; top: 0; left: 0; right: 0; z-index: 10000;">
            <a href="/" style="color: white; text-decoration: none; font-weight: bold; margin-right: 15px;">ğŸ  è¿”å›ä»£ç†ä¸»é¡µ</a>
            <span>å½“å‰ä»£ç†: {}</span>
        </div>
        '''.format(base_url[:50] + '...' if len(base_url) > 50 else base_url)

        # å¢å¼ºçš„é“¾æ¥é‡å†™ - ä¿®å¤æ–°é—»è·³è½¬ï¼Œä½†ä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥
        self._rewrite_links_complete(soup, base_url)

        # é‡å†™CSSä¸­çš„url()å¼•ç”¨
        style_tags = soup.find_all('style')
        for style_tag in style_tags:
            if style_tag.string:
                style_tag.string = self._rewrite_css_urls(style_tag.string, base_url)

        # ç¡®ä¿æœ‰æ­£ç¡®çš„å­—ç¬¦é›†å£°æ˜
        head_tag = soup.find('head')
        if head_tag:
            # ç§»é™¤ç°æœ‰çš„charsetå£°æ˜
            for meta in head_tag.find_all('meta', attrs={'charset': True}):
                meta.decompose()
            for meta in head_tag.find_all('meta', attrs={'http-equiv': lambda x: x and x.lower() == 'content-type'}):
                meta.decompose()
            
            # æ·»åŠ UTF-8 charsetå£°æ˜
            new_meta = soup.new_tag('meta', charset='utf-8')
            head_tag.insert(0, new_meta)
        else:
            # å¦‚æœæ²¡æœ‰headæ ‡ç­¾ï¼Œåˆ›å»ºä¸€ä¸ª
            head_tag = soup.new_tag('head')
            soup.insert(0, head_tag)
            new_meta = soup.new_tag('meta', charset='utf-8')
            head_tag.insert(0, new_meta)

        # æ’å…¥å¯¼èˆªæ å¹¶æ·»åŠ é¡¶éƒ¨è¾¹è·
        body_tag = soup.find('body')
        if body_tag:
            # ä¸ºbodyæ·»åŠ é¡¶éƒ¨è¾¹è·ä»¥å®¹çº³å›ºå®šå¯¼èˆªæ 
            body_style = body_tag.get('style', '')
            if 'margin-top' not in body_style:
                body_tag['style'] = body_style + '; margin-top: 50px;' if body_style else 'margin-top: 50px;'
            
            nav_soup = BeautifulSoup(nav_html, 'html.parser')
            body_tag.insert(0, nav_soup.div)
        else:
            # å¦‚æœæ²¡æœ‰bodyæ ‡ç­¾ï¼Œåˆ›å»ºä¸€ä¸ªå¹¶æ’å…¥å¯¼èˆªæ 
            body_tag = soup.new_tag('body')
            body_tag['style'] = 'margin-top: 50px;'
            nav_soup = BeautifulSoup(nav_html, 'html.parser')
            body_tag.append(nav_soup.div)
            
            # å°†åŸæœ‰å†…å®¹ç§»åŠ¨åˆ°bodyä¸­
            for content in soup.contents:
                if content.name != 'body':
                    body_tag.append(content)
            soup.append(body_tag)

        # æ³¨å…¥å¢å¼ºçš„JavaScriptæ‹¦æˆªä»£ç  - å…³é”®ä¿®å¤æ–°é—»è·³è½¬
        interception_script = self._get_enhanced_interception_script(base_url)
        script_tag = soup.new_tag('script')
        script_tag.string = interception_script
        if body_tag:
            body_tag.append(script_tag)
        else:
            soup.append(script_tag)

        # è¿”å›UTF-8ç¼–ç çš„å­—èŠ‚ä¸²
        return str(soup).encode('utf-8')

    def _rewrite_links_complete(self, soup, base_url):
        """å®Œæ•´çš„é“¾æ¥é‡å†™ - ä¿®å¤æ–°é—»è·³è½¬ï¼Œä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥"""
        # é‡å†™æ™®é€šé“¾æ¥ - è·³è¿‡è¿”å›ä¸»é¡µé“¾æ¥
        for tag in soup.find_all('a'):
            if tag.get('href'):
                href = tag['href']
                # ç‰¹åˆ«ä¿æŠ¤è¿”å›ä¸»é¡µçš„é“¾æ¥
                if href == '/' or href.startswith('/?'):
                    continue
                if self._should_rewrite_url(href):
                    absolute_url = urllib.parse.urljoin(base_url, href)
                    tag['href'] = "/proxy?url=" + urllib.parse.quote(absolute_url)

        # é‡å†™è¡¨å•
        for form in soup.find_all('form'):
            if form.get('action'):
                action = form['action']
                if self._should_rewrite_url(action):
                    absolute_url = urllib.parse.urljoin(base_url, action)
                    form['action'] = "/proxy?url=" + urllib.parse.quote(absolute_url)

        # é‡å†™èµ„æº
        for tag in soup.find_all(['img', 'script', 'link', 'iframe']):
            src_attr = 'src' if tag.get('src') else 'href' if tag.get('href') else None
            if src_attr:
                src = tag[src_attr]
                if self._should_rewrite_url(src):
                    absolute_url = urllib.parse.urljoin(base_url, src)
                    tag[src_attr] = "/proxy?url=" + urllib.parse.quote(absolute_url)

        # é‡å†™meta refresh
        for meta in soup.find_all('meta', attrs={'http-equiv': re.compile('refresh', re.I)}):
            content = meta.get('content', '')
            if 'url=' in content.lower():
                parts = content.split(';', 1)
                if len(parts) == 2:
                    timeout, url_part = parts
                    if url_part.strip().lower().startswith('url='):
                        original_url = url_part[4:].strip()
                        if self._should_rewrite_url(original_url):
                            absolute_url = urllib.parse.urljoin(base_url, original_url)
                            proxy_url = "/proxy?url=" + urllib.parse.quote(absolute_url)
                            meta['content'] = f"{timeout}; URL={proxy_url}"

    def _should_rewrite_url(self, url):
        """åˆ¤æ–­URLæ˜¯å¦éœ€è¦é‡å†™"""
        if not url or url.strip() == '':
            return False
        if url.startswith(('#', 'javascript:', 'mailto:', 'tel:', 'data:', 'blob:')):
            return False
        if url.startswith(('/proxy?url=', '/resource/', 'http://localhost:', 'https://localhost:')):
            return False
        if url == '/' or url.startswith('/?'):
            return False
        return True

    def _rewrite_css_urls(self, css_content, base_url):
        """é‡å†™CSSä¸­çš„url()å¼•ç”¨"""
        import re

        def replace_url(match):
            url_content = match.group(1)
            if url_content.startswith(('http://', 'https://', 'data:')):
                return match.group(0)

            absolute_url = urllib.parse.urljoin(base_url, url_content.strip('"\''))
            proxy_url = "/proxy?url=" + urllib.parse.quote(absolute_url)
            return 'url("' + proxy_url + '")'

        pattern = r'url\(["\']?([^"\'()]*)["\']?\)'
        return re.sub(pattern, replace_url, css_content)

    def _get_enhanced_interception_script(self, base_url):
        """è·å–å¢å¼ºçš„JavaScriptæ‹¦æˆªä»£ç  - å½»åº•ä¿®å¤æ–°é—»è·³è½¬"""
        return '''
        // å¢å¼ºçš„JavaScriptæ‹¦æˆªä»£ç  - ä¸“é—¨ä¿®å¤æ–°é—»è·³è½¬
        (function() {
            'use strict';
            
            var baseUrl = "''' + base_url + '''";
            
            // 1. å¢å¼ºçš„ç‚¹å‡»äº‹ä»¶æ‹¦æˆª - å¤„ç†æ‰€æœ‰å¯èƒ½çš„æ–°é—»é“¾æ¥
            function interceptClickEvent(e) {
                var target = e.target;
                
                // å‘ä¸Šéå†æ‰€æœ‰çˆ¶å…ƒç´ ï¼ŒæŸ¥æ‰¾é“¾æ¥
                while (target && target !== document) {
                    if (target.tagName && target.tagName.toLowerCase() === 'a' && target.href) {
                        var href = target.href;
                        
                        // æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦ä»£ç†çš„å¤–éƒ¨é“¾æ¥ï¼ˆç‰¹åˆ«ä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥ï¼‰
                        if (href && 
                            !href.includes('/proxy?url=') && 
                            !href.startsWith('javascript:') && 
                            !href.startsWith('mailto:') && 
                            !href.startsWith('tel:') && 
                            !href.startsWith('#') &&
                            !href.startsWith('data:') &&
                            !(target.getAttribute('href') === '/') && // ä¿æŠ¤è¿”å›ä¸»é¡µé“¾æ¥
                            !(target.textContent && target.textContent.includes('è¿”å›ä»£ç†ä¸»é¡µ'))) {
                            
                            e.preventDefault();
                            e.stopImmediatePropagation();
                            e.stopPropagation();
                            
                            // è½¬æ¢ä¸ºä»£ç†URL
                            try {
                                var fullUrl = new URL(href, baseUrl).href;
                                var proxyUrl = '/proxy?url=' + encodeURIComponent(fullUrl);
                                window.location.href = proxyUrl;
                            } catch (err) {
                                console.log('æ‹¦æˆªé”™è¯¯:', err);
                            }
                            return false;
                        }
                    }
                    target = target.parentNode;
                }
            }
            
            // 2. å¤šå±‚çº§äº‹ä»¶ç›‘å¬ - ç¡®ä¿æ•è·æ‰€æœ‰ç‚¹å‡»
            document.addEventListener('click', interceptClickEvent, true);
            document.addEventListener('auxclick', interceptClickEvent, true); // ä¸­é”®ç‚¹å‡»
            document.addEventListener('contextmenu', interceptClickEvent, true); // å³é”®èœå•
            
            // 3. æ‹¦æˆªåŠ¨æ€åˆ›å»ºçš„é“¾æ¥
            var observer = new MutationObserver(function(mutations) {
                mutations.forEach(function(mutation) {
                    mutation.addedNodes.forEach(function(node) {
                        if (node.nodeType === 1) { // Element node
                            // æ£€æŸ¥æ–°å¢çš„é“¾æ¥
                            if (node.tagName === 'A' && node.href && !node.href.includes('/proxy?url=')) {
                                var fullUrl = new URL(node.href, baseUrl).href;
                                node.href = '/proxy?url=' + encodeURIComponent(fullUrl);
                            }
                            
                            // æ£€æŸ¥å­å…ƒç´ ä¸­çš„é“¾æ¥
                            var links = node.querySelectorAll ? node.querySelectorAll('a') : [];
                            for (var i = 0; i < links.length; i++) {
                                var link = links[i];
                                if (link.href && !link.href.includes('/proxy?url=')) {
                                    var fullUrl = new URL(link.href, baseUrl).href;
                                    link.href = '/proxy?url=' + encodeURIComponent(fullUrl);
                                }
                            }
                        }
                    });
                });
            });
            
            // 4. å¼€å§‹è§‚å¯ŸDOMå˜åŒ–
            observer.observe(document.body, {
                childList: true,
                subtree: true
            });
            
            // 5. æ‹¦æˆªwindow.openç­‰æ–¹æ³•
            var originalWindowOpen = window.open;
            window.open = function(url, target, features) {
                if (url && !url.includes('/proxy?url=')) {
                    var fullUrl = new URL(url, baseUrl).href;
                    url = '/proxy?url=' + encodeURIComponent(fullUrl);
                }
                return originalWindowOpen.call(this, url, target, features);
            };
            
            console.log('ğŸ”’ å¢å¼ºæ‹¦æˆªè„šæœ¬å·²åŠ è½½ - æ–°é—»è·³è½¬å·²ä¿®å¤');
        })();
        '''

    def log_message(self, format, *args):
        """è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼"""
        print("[" + self.log_date_time_string() + "] " + format % args)

def run_proxy_server():
    """è¿è¡Œä»£ç†æœåŠ¡å™¨"""
    port = 60000
    
    try:
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    except:
        pass
    
    with socketserver.TCPServer(("", port), SoComFixProxyHandler) as httpd:
        print("ğŸš€ 360æœç´¢ä¹±ç ä¿®å¤ä»£ç†æœåŠ¡å™¨å·²å¯åŠ¨åœ¨ç«¯å£ " + str(port))
        print("ğŸ“§ è®¿é—®åœ°å€: http://localhost:" + str(port))
        print("ğŸ”§ ä¿®å¤å†…å®¹: 360æœç´¢ä¹±ç  + æ–°é—»ç›´æ¥è·³è½¬ + æœç´¢é—®é¢˜ + ä¸‹è½½æ–‡ä»¶ + 412é”™è¯¯")
        print("â¹ï¸ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nğŸ›‘ æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    run_proxy_server()
