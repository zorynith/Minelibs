#!/usr/bin/env python3
"""
HTTPä»£ç†æœåŠ¡å™¨ - ä¸“é—¨ä¿®å¤æœç´¢é—®é¢˜ç‰ˆæœ¬
è¿è¡Œç«¯å£: 60000
ä¿æŒä¸»é¡µå¾ªç¯ä¿®å¤ï¼Œä¸“é—¨è§£å†³æœç´¢é—®é¢˜
"""

import http.server
import socketserver
import urllib.parse
import requests
from bs4 import BeautifulSoup
import re
import time

class SearchFixProxyHandler(http.server.BaseHTTPRequestHandler):
    """ä¸“é—¨ä¿®å¤æœç´¢é—®é¢˜çš„ä»£ç†å¤„ç†å™¨"""
    
    config = {
        'port': 60000,
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    def do_GET(self):
        """å¤„ç†GETè¯·æ±‚"""
        try:
            print(f"GETè¯·æ±‚: {self.path}")
            
            if self.path == '/':
                self._serve_homepage()
            elif self.path.startswith('/proxy?url='):
                self._proxy_webpage()
            elif self.path.startswith('/proxy?') and 'url=' not in self.path:
                # å…³é”®ä¿®å¤ï¼šå¤„ç†æœç´¢åç”Ÿæˆçš„URLï¼ˆæ²¡æœ‰urlå‚æ•°çš„æƒ…å†µï¼‰
                self._handle_search_result()
            else:
                self._proxy_resource()
        except Exception as e:
            self.send_error(500, "Server Error: " + str(e))
    
    def do_POST(self):
        """å¤„ç†POSTè¯·æ±‚"""
        try:
            if self.path.startswith('/proxy?url='):
                self._proxy_post_request_fixed()
            else:
                self.send_error(404, "Not Found")
        except Exception as e:
            self.send_error(500, "Server Error: " + str(e))
    
    def _handle_search_result(self):
        """ä¸“é—¨å¤„ç†æœç´¢åç”Ÿæˆçš„URL"""
        try:
            # è§£æå½“å‰è·¯å¾„ä¸­çš„å‚æ•°
            parsed_path = urllib.parse.urlparse(self.path)
            query_params = urllib.parse.parse_qs(parsed_path.query)
            
            print(f"æœç´¢å‚æ•°: {query_params}")
            
            # ä»Refererä¸­è·å–åŸå§‹æœç´¢é¡µé¢URL
            referer = self.headers.get('Referer', '')
            if referer and '/proxy?url=' in referer:
                # æå–åŸå§‹æœç´¢é¡µé¢çš„URL
                referer_parsed = urllib.parse.urlparse(referer)
                referer_query = urllib.parse.parse_qs(referer_parsed.query)
                base_search_url = referer_query.get('url', [''])[0]
                
                if base_search_url:
                    # æ„å»ºæœç´¢ç»“æœçš„å®Œæ•´URL
                    # å¯¹äº360æœç´¢ï¼Œæœç´¢ç»“æœé¡µé€šå¸¸æ˜¯ www.so.com/s åŠ ä¸Šå‚æ•°
                    search_result_url = "https://www.so.com/s"
                    
                    # æ·»åŠ æ‰€æœ‰æœç´¢å‚æ•°
                    if query_params:
                        search_result_url += "?" + urllib.parse.urlencode(query_params, doseq=True)
                    
                    print(f"æ„å»ºæœç´¢ç»“æœURL: {search_result_url}")
                    
                    # ä»£ç†è¿™ä¸ªæœç´¢ç»“æœé¡µé¢
                    self._proxy_specific_url(search_result_url)
                    return
            
            # å¦‚æœæ— æ³•ä»Refererè·å–ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            self._try_auto_fix_search()
            
        except Exception as e:
            print(f"æœç´¢å¤„ç†é”™è¯¯: {str(e)}")
            self.send_error(500, "Search processing error: " + str(e))
    
    def _try_auto_fix_search(self):
        """å°è¯•è‡ªåŠ¨ä¿®å¤æœç´¢URL"""
        try:
            # è§£æå½“å‰è·¯å¾„
            parsed_path = urllib.parse.urlparse(self.path)
            query_params = urllib.parse.parse_qs(parsed_path.query)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœç´¢å‚æ•°
            if 'q' in query_params or 'query' in query_params or 'keyword' in query_params:
                # å‡è®¾æ˜¯360æœç´¢
                search_result_url = "https://www.so.com/s"
                
                if query_params:
                    search_result_url += "?" + urllib.parse.urlencode(query_params, doseq=True)
                
                print(f"è‡ªåŠ¨ä¿®å¤æœç´¢URL: {search_result_url}")
                self._proxy_specific_url(search_result_url)
                return
            
            # å¦‚æœæ— æ³•ä¿®å¤ï¼Œè¿”å›é”™è¯¯
            self.send_error(400, "æ— æ³•å¤„ç†çš„æœç´¢è¯·æ±‚")
            
        except Exception as e:
            print(f"è‡ªåŠ¨ä¿®å¤é”™è¯¯: {str(e)}")
            self.send_error(500, "Auto-fix error: " + str(e))
    
    def _proxy_specific_url(self, target_url):
        """ä»£ç†ç‰¹å®šURL"""
        print(f"ä»£ç†ç‰¹å®šURL: {target_url}")
        
        headers = {
            'User-Agent': self.config['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
        
        try:
            response = requests.get(target_url, headers=headers, timeout=30, verify=False)
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to fetch: " + str(e))
            return
        
        if response.status_code != 200:
            self.send_error(response.status_code, "Target returned " + str(response.status_code))
            return
        
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' in content_type:
            rewritten_content = self._rewrite_html_content_fixed(response.text, target_url)
            self.send_response(200)
            self.send_header('Content-Type', 'text/html; charset=utf-8')
            self.send_header('Content-Length', str(len(rewritten_content.encode('utf-8'))))
            self.end_headers()
            self.wfile.write(rewritten_content.encode('utf-8'))
        else:
            self._proxy_raw_content(response)
    
    def _serve_homepage(self):
        """æä¾›ä»£ç†ä¸»é¡µç•Œé¢"""
        homepage_html = '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>æœç´¢ä¿®å¤ä»£ç† - 60000ç«¯å£</title>
            <meta charset="utf-8">
            <style>
                body { 
                    font-family: Arial, sans-serif; 
                    margin: 40px; 
                    background: #f5f5f5;
                }
                .container { 
                    max-width: 800px; 
                    margin: 0 auto; 
                    background: white; 
                    padding: 30px; 
                    border-radius: 10px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }
                h1 { 
                    color: #333; 
                    text-align: center;
                }
                .form-group { 
                    margin: 20px 0; 
                }
                input[type="url"] { 
                    width: 100%; 
                    padding: 12px; 
                    border: 1px solid #ddd; 
                    border-radius: 5px; 
                    font-size: 16px;
                    box-sizing: border-box;
                }
                button { 
                    background: #007cba; 
                    color: white; 
                    border: none; 
                    padding: 12px 24px; 
                    border-radius: 5px; 
                    cursor: pointer; 
                    font-size: 16px;
                    width: 100%;
                }
                button:hover { 
                    background: #005a87; 
                }
                .info {
                    background: #e7f3ff;
                    padding: 15px;
                    border-radius: 5px;
                    margin: 20px 0;
                    border-left: 4px solid #007cba;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸŒ æœç´¢ä¿®å¤ä»£ç†æœåŠ¡</h1>
                <div class="info">
                    <strong>ä¿®å¤å†…å®¹ï¼š</strong> ä¿æŒä¸»é¡µå¾ªç¯ä¿®å¤ï¼Œä¸“é—¨è§£å†³æœç´¢æ¡†æäº¤é—®é¢˜ã€‚
                </div>
                <form action="/proxy" method="GET">
                    <div class="form-group">
                        <input type="url" name="url" placeholder="https://www.example.com" required value="https://www.so.com">
                    </div>
                    <button type="submit">å¼€å§‹ä»£ç†è®¿é—®</button>
                </form>
                <div style="margin-top: 20px; text-align: center; color: #666;">
                    <small>ä»£ç†æœåŠ¡è¿è¡Œåœ¨ç«¯å£ 60000 | æœç´¢é—®é¢˜ä¸“é—¨ä¿®å¤</small>
                </div>
            </div>
        </body>
        </html>
        '''
        
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.send_header('Content-Length', str(len(homepage_html.encode('utf-8'))))
        self.end_headers()
        self.wfile.write(homepage_html.encode('utf-8'))
    
    def _proxy_webpage(self):
        """ä»£ç†å¹¶é‡å†™ç½‘é¡µå†…å®¹"""
        query_params = urllib.parse.parse_qs(urllib.parse.urlparse(self.path).query)
        target_url = query_params.get('url', [''])[0]
        
        if not target_url:
            self.send_error(400, "Missing URL parameter")
            return
        
        if not target_url.startswith(('http://', 'https://')):
            target_url = 'http://' + target_url
        
        headers = {
            'User-Agent': self.config['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        try:
            response = requests.get(target_url, headers=headers, timeout=30, verify=False)
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to fetch target website: " + str(e))
            return
        
        if response.status_code != 200:
            self.send_error(response.status_code, "Target website returned " + str(response.status_code))
            return
        
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' in content_type:
            rewritten_content = self._rewrite_html_content_fixed(response.text, target_url)
            self.send_response(200)
            self.send_header('Content-Type', 'text/html; charset=utf-8')
            self.send_header('Content-Length', str(len(rewritten_content.encode('utf-8'))))
            self.end_headers()
            self.wfile.write(rewritten_content.encode('utf-8'))
        else:
            self._proxy_raw_content(response)
    
    def _proxy_resource(self):
        """ä»£ç†é™æ€èµ„æº"""
        referer = self.headers.get('Referer', '')
        
        if '/proxy?url=' in referer:
            referer_parsed = urllib.parse.urlparse(referer)
            referer_query = urllib.parse.parse_qs(referer_parsed.query)
            base_url = referer_query.get('url', [''])[0]
            
            if base_url:
                resource_url = urllib.parse.urljoin(base_url, self.path)
                
                headers = {
                    'User-Agent': self.config['user_agent'],
                    'Referer': base_url
                }
                
                try:
                    response = requests.get(resource_url, headers=headers, timeout=30, verify=False)
                    self._proxy_raw_content(response)
                    return
                except requests.exceptions.RequestException:
                    pass
        
        self.send_error(404, "Resource not found")
    
    def _proxy_post_request_fixed(self):
        """ä¿®å¤çš„POSTè¯·æ±‚å¤„ç†"""
        query_params = urllib.parse.parse_qs(urllib.parse.urlparse(self.path).query)
        target_url = query_params.get('url', [''])[0]
        
        if not target_url:
            self.send_error(400, "Missing URL parameter")
            return
        
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length) if content_length > 0 else b''
        
        headers = {
            'User-Agent': self.config['user_agent'],
            'Content-Type': self.headers.get('Content-Type', 'application/x-www-form-urlencoded'),
        }
        
        try:
            response = requests.post(target_url, data=post_data, headers=headers, 
                                   timeout=30, verify=False, allow_redirects=False)
            
            print(f"POSTå“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code in [301, 302, 303, 307, 308]:
                location = response.headers.get('Location', '')
                if location:
                    print(f"æ‹¦æˆªåˆ°é‡å®šå‘: {location}")
                    
                    if not location.startswith(('http://', 'https://')):
                        location = urllib.parse.urljoin(target_url, location)
                    
                    proxy_location = "/proxy?url=" + urllib.parse.quote(location)
                    print(f"é‡å†™ä¸ºä»£ç†é‡å®šå‘: {proxy_location}")
                    
                    self.send_response(302)
                    self.send_header('Location', proxy_location)
                    self.end_headers()
                    return
            
            self._proxy_raw_content(response)
            
        except requests.exceptions.RequestException as e:
            self.send_error(502, "Failed to POST to target website: " + str(e))
    
    def _proxy_raw_content(self, response):
        """ä»£ç†åŸå§‹å†…å®¹"""
        self.send_response(response.status_code)
        
        for header, value in response.headers.items():
            if header.lower() not in ['content-encoding', 'transfer-encoding', 'content-length']:
                self.send_header(header, value)
        
        self.send_header('Content-Length', str(len(response.content)))
        self.end_headers()
        self.wfile.write(response.content)
    
    def _rewrite_html_content_fixed(self, html_content, base_url):
        """ä¿®å¤çš„HTMLå†…å®¹é‡å†™"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        nav_html = '''
        <div style="background: #007cba; color: white; padding: 10px; margin: 0; text-align: center;">
            <a href="/" style="color: white; text-decoration: none; font-weight: bold;">ğŸ  è¿”å›ä»£ç†ä¸»é¡µ</a>
            <span style="margin: 0 10px;">|</span>
            <span>å½“å‰ä»£ç†: {}</span>
        </div>
        '''.format(base_url)
        
        self._rewrite_attributes_fixed(soup, 'a', 'href', base_url)
        self._rewrite_attributes_fixed(soup, 'img', 'src', base_url)
        self._rewrite_attributes_fixed(soup, 'script', 'src', base_url)
        self._rewrite_attributes_fixed(soup, 'link', 'href', base_url)
        self._rewrite_attributes_fixed(soup, 'form', 'action', base_url)
        self._rewrite_attributes_fixed(soup, 'iframe', 'src', base_url)
        
        style_tags = soup.find_all('style')
        for style_tag in style_tags:
            if style_tag.string:
                style_tag.string = self._rewrite_css_urls(style_tag.string, base_url)
        
        body_tag = soup.find('body')
        if body_tag:
            nav_soup = BeautifulSoup(nav_html, 'html.parser')
            body_tag.insert(0, nav_soup.div)
        else:
            soup.insert(0, BeautifulSoup(nav_html, 'html.parser').div)
        
        return str(soup)
    
    def _rewrite_attributes_fixed(self, soup, tag_name, attr_name, base_url):
        """ä¿®å¤çš„å±æ€§é‡å†™"""
        tags = soup.find_all(tag_name)
        for tag in tags:
            if tag.has_attr(attr_name):
                original_url = tag[attr_name]
                
                if not original_url or original_url.startswith(('#', 'javascript:', 'mailto:')):
                    continue
                
                # å…³é”®ä¿®å¤ï¼šè·³è¿‡ä¸»é¡µé“¾æ¥ï¼Œé˜²æ­¢å¾ªç¯ä»£ç†
                if original_url == '/' or original_url.startswith('/?'):
                    continue
                
                absolute_url = urllib.parse.urljoin(base_url, original_url)
                proxy_url = "/proxy?url=" + urllib.parse.quote(absolute_url)
                tag[attr_name] = proxy_url
    
    def _rewrite_css_urls(self, css_content, base_url):
        """é‡å†™CSSä¸­çš„url()å¼•ç”¨"""
        import re
        
        def replace_url(match):
            url_content = match.group(1)
            if url_content.startswith(('http://', 'https://', 'data:')):
                return match.group(0)
            
            absolute_url = urllib.parse.urljoin(base_url, url_content.strip('"\''))
            proxy_url = "/proxy?url=" + urllib.parse.quote(absolute_url)
            return 'url("' + proxy_url + '")'
        
        pattern = r'url\(["\']?([^"\'()]*)["\']?\)'
        return re.sub(pattern, replace_url, css_content)
    
    def log_message(self, format, *args):
        """è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼"""
        print("[" + self.log_date_time_string() + "] " + format % args)

def run_proxy_server():
    """è¿è¡Œä»£ç†æœåŠ¡å™¨"""
    port = 60000
    
    try:
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    except:
        pass
    
    with socketserver.TCPServer(("", port), SearchFixProxyHandler) as httpd:
        print("ğŸš€ æœç´¢ä¿®å¤ä»£ç†æœåŠ¡å™¨å·²å¯åŠ¨åœ¨ç«¯å£ " + str(port))
        print("ğŸ“§ è®¿é—®åœ°å€: http://localhost:" + str(port))
        print("ğŸ”§ ä¿®å¤å†…å®¹: ä¸»é¡µå¾ªç¯ + æœç´¢é—®é¢˜ä¸“é—¨å¤„ç†")
        print("â¹ï¸ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nğŸ›‘ æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    run_proxy_server()
